1
00:00:00,000 --> 00:00:03,253
Way back in the 80s, I noticed that

2
00:00:03,253 --> 00:00:05,505
sometimes, when an elephant

3
00:00:05,505 --> 00:00:08,675
called a member of her family,

4
00:00:08,675 --> 00:00:12,178
one individual would answer,

5
00:00:12,178 --> 00:00:14,806
and everybody else ignored the calling animal.

6
00:00:14,806 --> 00:00:17,934
And then she would call again,

7
00:00:17,934 --> 00:00:20,311
and a different elephant would sort of lift

8
00:00:20,311 --> 00:00:24,399
her head up and rumble very loudly.

9
00:00:24,399 --> 00:00:25,692
This is Joyce Poole.

10
00:00:25,692 --> 00:00:27,485
She's been studying African elephants

11
00:00:27,485 --> 00:00:30,030
and their communication for 50 years.

12
00:00:30,030 --> 00:00:31,990
Then I started to think, well, okay,

13
00:00:31,990 --> 00:00:36,745
so maybe they have a way of directing a call to a specific individual.

14
00:00:36,745 --> 00:00:39,164
But we had no way of detecting that.

15
00:00:39,164 --> 00:00:41,791
Decades later, she partnered up with Mickey Pardo,

16
00:00:41,791 --> 00:00:44,419
who designed a study around her observations.

17
00:00:44,419 --> 00:00:45,420
I went out into the field.

18
00:00:45,420 --> 00:00:48,381
I recorded calls with careful behavioral observations.

19
00:00:48,381 --> 00:00:50,717
So we knew who made each call, we knew who the call was

20
00:00:50,717 --> 00:00:53,136
addressed to, we knew the context of the call...

21
00:00:53,136 --> 00:00:56,014
They encoded the acoustic information from the recordings

22
00:00:56,014 --> 00:00:58,600
into a long string of numbers, along with the data

23
00:00:58,600 --> 00:01:01,186
Mickey collected about the calls.

24
00:01:01,186 --> 00:01:03,104
They fed nearly 500 different

25
00:01:03,104 --> 00:01:05,690
calls like this into a statistical model.

26
00:01:05,690 --> 00:01:08,985
And when given the acoustic structure of a new call,

27
00:01:08,985 --> 00:01:11,738
the model could predict who the receiver of the call was,

28
00:01:11,738 --> 00:01:13,740
much better than chance.

29
00:01:13,740 --> 00:01:15,909
In other words, evidence suggesting

30
00:01:15,909 --> 00:01:20,580
African savanna elephants give each other names.

31
00:01:20,580 --> 00:01:22,415
When we posted it on Facebook,

32
00:01:22,415 --> 00:01:27,754
somebody wrote back, said that the Earth just shifted a little bit.

33
00:01:27,754 --> 00:01:30,590
And I think that's true.

34
00:01:30,590 --> 00:01:32,801
This is just one example of how machine learning

35
00:01:32,801 --> 00:01:37,138
is decoding complexities in animal communication
that humans can't detect.

36
00:01:37,138 --> 00:01:40,266
And now some AI researchers want to take the next step:

37
00:01:40,266 --> 00:01:43,520
Large language models, like the ones that power chatbots,

38
00:01:43,520 --> 00:01:47,107
but built for interspecies communication.

39
00:01:47,107 --> 00:01:49,317
“Can we talk a little bit about love?”

40
00:01:49,317 --> 00:01:52,195
“There is still much to be learned about whales.”

41
00:02:05,166 --> 00:02:07,127
When researchers study animal communication,

42
00:02:07,127 --> 00:02:09,211
they usually employ a few methods:

43
00:02:09,211 --> 00:02:11,548
Recording their vocalizations, observing

44
00:02:11,548 --> 00:02:14,676
and documenting the behavior and context around those sounds,

45
00:02:14,676 --> 00:02:18,429
and sometimes doing a playback to measure the animal's response.

46
00:02:18,429 --> 00:02:22,100
All of these areas are already being impacted by AI.

47
00:02:22,100 --> 00:02:27,897
Recordings from the field don't usually sound like this.

48
00:02:27,897 --> 00:02:32,443
They often sound like this:

49
00:02:32,443 --> 00:02:33,570
Multiple animals

50
00:02:33,570 --> 00:02:36,948
vocalizing on top of one another in a noisy environment.

51
00:02:36,948 --> 00:02:39,075
This is known as the cocktail party problem,

52
00:02:39,075 --> 00:02:42,370
and it's a common issue in the field of animal research.

53
00:02:42,370 --> 00:02:46,583
But machine learning solved a similar problem
in human speech recognition.

54
00:02:46,583 --> 00:02:49,586
AI researchers trained a model called Deep Karaoke on

55
00:02:49,586 --> 00:02:52,088
lots of music tracks where instruments and vocals

56
00:02:52,088 --> 00:02:56,050
were recorded separately, then also on the fully mixed tracks,

57
00:02:56,050 --> 00:02:59,387
until it was able to do the task of separating out instruments

58
00:02:59,387 --> 00:03:02,348
and vocals in new music clips.

59
00:03:02,348 --> 00:03:05,018
Recently, AI researchers have had some success

60
00:03:05,018 --> 00:03:08,438
applying similar algorithms to animal sound recordings.

61
00:03:08,438 --> 00:03:11,941
Which means you can take a clip of a group of macaque monkeys,

62
00:03:11,941 --> 00:03:18,406
and single out one discernable call.

63
00:03:18,406 --> 00:03:20,325
Researchers could also start using

64
00:03:20,325 --> 00:03:22,869
AI in how they use playbacks in the field.

65
00:03:22,869 --> 00:03:25,205
You may have seen AI models that can be trained

66
00:03:25,205 --> 00:03:28,875
on lots of examples of a sound recording

67
00:03:31,628 --> 00:03:35,256
and then generate another unique version of it.

68
00:03:38,509 --> 00:03:39,969
AI researchers are starting

69
00:03:39,969 --> 00:03:45,558
to develop similar models for animal recordings.

70
00:03:45,558 --> 00:03:48,436
These are all types of “supervised learning.”

71
00:03:48,436 --> 00:03:49,979
That means that the model gets trained

72
00:03:49,979 --> 00:03:52,482
on lots of examples labeled by humans.

73
00:03:52,482 --> 00:03:54,692
And in the elephant name study, researchers were able

74
00:03:54,692 --> 00:03:57,195
to feed a model their observations, which,

75
00:03:57,195 --> 00:03:59,322
along with the sound data, helped them detect

76
00:03:59,322 --> 00:04:02,492
something in elephant calls they couldn't through observation alone.

77
00:04:02,492 --> 00:04:04,285
You need to annotate a lot of data.

78
00:04:04,285 --> 00:04:06,746
Yossi Yovel trained a statistical model on

79
00:04:06,746 --> 00:04:09,874
15,000 Egyptian fruit bat vocalizations,

80
00:04:09,874 --> 00:04:12,293
and it was able to identify the emitter of the call,

81
00:04:12,293 --> 00:04:13,920
the context of the call,

82
00:04:13,920 --> 00:04:17,257
its behavioral response, and who the call was addressed to.

83
00:04:17,257 --> 00:04:19,634
And we annotated them manually.

84
00:04:19,634 --> 00:04:22,720
You know, I'm already saying this is a restriction of the study,

85
00:04:22,720 --> 00:04:25,807
because maybe we're missing something, we’re humans, we’re not bats.

86
00:04:25,807 --> 00:04:28,726
And that's the problem with supervised learning models.

87
00:04:28,726 --> 00:04:31,145
They are limited by what we humans already know

88
00:04:31,145 --> 00:04:34,732
about animal communication in order to label the training data.

89
00:04:34,732 --> 00:04:36,818
And we don't know a lot.

90
00:04:36,818 --> 00:04:40,154
That's why some AI researchers say self-supervised models

91
00:04:40,154 --> 00:04:43,366
hold the most potential for decoding animal communication.

92
00:04:43,366 --> 00:04:47,287
This is how natural language processing models like ChatGPT are trained.

93
00:04:47,287 --> 00:04:49,330
Instead of human-labeled examples,

94
00:04:49,330 --> 00:04:52,625
they are trained on a large amount of unlabeled data,

95
00:04:52,625 --> 00:04:55,545
and then can sort it according to patterns and categories

96
00:04:55,545 --> 00:04:57,505
it detects all on its own.

97
00:04:57,505 --> 00:05:00,633
In the example of ChatGPT, it learned from all the books,

98
00:05:00,633 --> 00:05:03,052
websites, social media feeds and anything else

99
00:05:03,052 --> 00:05:04,512
it could scrape from the internet,

100
00:05:04,512 --> 00:05:07,515
and came to its own conclusions about how language works.

101
00:05:07,515 --> 00:05:11,936
Every language has a shape that AI discovers.

102
00:05:11,936 --> 00:05:13,187
This is Aza Raskin.

103
00:05:13,187 --> 00:05:15,398
He co-founded the Earth Species Project,

104
00:05:15,398 --> 00:05:17,650
one of a few organizations that want to build

105
00:05:17,650 --> 00:05:20,028
models like this for animal communication.

106
00:05:20,028 --> 00:05:22,655
What he means by language having a shape,

107
00:05:22,655 --> 00:05:26,743
is that language models are built out of relationships among words.

108
00:05:26,743 --> 00:05:29,245
Words that mean similar things are placed near each other, words

109
00:05:29,245 --> 00:05:33,624
that share a relationship, share a distance and direction.

110
00:05:33,624 --> 00:05:36,127
So, man is to king as woman is to queen.

111
00:05:36,127 --> 00:05:38,796
So this is the shape of all those relationships

112
00:05:38,796 --> 00:05:41,382
among the English language’s 10,000 most common

113
00:05:41,382 --> 00:05:44,385
words, visualized here by the Earth Species Project.

114
00:05:44,385 --> 00:05:46,846
Flattened out it looks something like this.

115
00:05:46,846 --> 00:05:51,851
Something really miraculous happened in 2017, and that was, researchers

116
00:05:51,851 --> 00:05:54,979
discovered that you could take the shape

117
00:05:54,979 --> 00:05:57,190
of any one language

118
00:05:57,190 --> 00:06:00,735
and match it to the shape of any other language,

119
00:06:00,735 --> 00:06:03,780
and the point which is “dog” ends up in the same spot.

120
00:06:03,780 --> 00:06:07,825
This idea, that similar words can be located
in other languages in roughly

121
00:06:07,825 --> 00:06:10,870
the same place, is what gives the Earth Species Project

122
00:06:10,870 --> 00:06:14,165
hope we could do a version of this for animal communication.

123
00:06:14,165 --> 00:06:17,168
To do a translation without needing any examples,

124
00:06:17,168 --> 00:06:19,670
without needing a Rosetta stone.

125
00:06:19,670 --> 00:06:22,048
This is complicated though, because we know that animals

126
00:06:22,048 --> 00:06:25,259
don't just communicate with sound, but with other senses, too.

127
00:06:25,259 --> 00:06:27,512
But Aza points out that we can learn from the fact

128
00:06:27,512 --> 00:06:30,390
that image generation models like DALL-E and Midjourney

129
00:06:30,390 --> 00:06:34,602
are built on the same large language model structure used for text.

130
00:06:34,602 --> 00:06:38,606
It turns out, behind the scenes, it's again these kinds of shapes.

131
00:06:38,606 --> 00:06:43,069
There's the shape that represents
sound, the shape that represents images.

132
00:06:43,069 --> 00:06:46,364
Those two shapes get aligned, and now you can translate

133
00:06:46,364 --> 00:06:50,410
between images and text.

134
00:06:50,410 --> 00:06:52,620
Their expectation is that where nonhuman

135
00:06:52,620 --> 00:06:55,164
animals’ communication would line up with ours

136
00:06:55,164 --> 00:06:58,042
will tell us even more about what we have in common.

137
00:06:58,042 --> 00:07:01,421
Dolphins look in mirrors and recognize themselves.

138
00:07:01,421 --> 00:07:02,255
Elephants too.

139
00:07:02,255 --> 00:07:04,298
That's a kind of self-awareness.

140
00:07:04,298 --> 00:07:06,676
One concern with this plan is related to a step

141
00:07:06,676 --> 00:07:09,554
in self-supervised learning called validation,

142
00:07:09,554 --> 00:07:11,764
meaning humans still need to refine these models

143
00:07:11,764 --> 00:07:14,058
by grading them on their answers.

144
00:07:14,058 --> 00:07:18,229
How would we do that in a communication so foreign from our own?

145
00:07:18,229 --> 00:07:21,732
We also might have too high expectations of this overlap,

146
00:07:21,732 --> 00:07:23,943
or the capacity for having a conversation

147
00:07:23,943 --> 00:07:28,114
with a nonhuman animal in a shared language,
and about shared experiences.

148
00:07:28,114 --> 00:07:30,074
“Hey Kurt, how are you doing dude?”

149
00:07:30,074 --> 00:07:32,618
“So I'm about to translate that into a meow.”

150
00:07:34,328 --> 00:07:35,830
“We said hi. Hi.

151
00:07:35,830 --> 00:07:37,331
Hi. Hi.

152
00:07:37,331 --> 00:07:40,501
You know, next time you want to say, how are you?”

153
00:07:40,501 --> 00:07:42,879
I do not think that humans should be considered

154
00:07:42,879 --> 00:07:46,215
more important than other species, but that doesn't mean

155
00:07:46,215 --> 00:07:50,052
that there's no usefulness in distinguishing between language,

156
00:07:50,052 --> 00:07:53,097
which is this very specific behavior that, at least

157
00:07:53,097 --> 00:07:57,185
based on what we currently know, seems to be unique to humans,

158
00:07:57,185 --> 00:07:59,353
and other forms of communication.

159
00:07:59,353 --> 00:08:02,064
In order to build these models, the first step is collecting

160
00:08:02,064 --> 00:08:05,651
a lot more data on animal sounds than exists right now.

161
00:08:05,651 --> 00:08:09,697
And so I'm actually at the moment building up a database

162
00:08:09,697 --> 00:08:12,158
with all the individual calls.

163
00:08:12,158 --> 00:08:15,286
So, close to 10,000 records in that,

164
00:08:15,286 --> 00:08:18,372
which is very small actually.

165
00:08:18,372 --> 00:08:19,957
Around the world, animal researchers

166
00:08:19,957 --> 00:08:22,668
are in the midst of a massive data collection effort,

167
00:08:22,668 --> 00:08:25,630
tagging and recording animals with video and sound

168
00:08:25,630 --> 00:08:28,758
and spatial data to feed these data-thirsty models.

169
00:08:28,758 --> 00:08:30,760
Time will tell whether true interspecies

170
00:08:30,760 --> 00:08:33,554
communication will be facilitated by AI.

171
00:08:33,554 --> 00:08:35,556
But researchers hope that discoveries along

172
00:08:35,556 --> 00:08:38,683
the way will continue to have an impact on our appreciation

173
00:08:38,683 --> 00:08:42,354
and protection of the species we share the planet with.

174
00:08:42,355 --> 00:08:46,692
We're not the only ones on the planet who can communicate,

175
00:08:46,692 --> 00:08:51,614
who care about one another, who have thoughts about the past

176
00:08:51,614 --> 00:08:52,949
and about the future.

177
00:08:52,949 --> 00:08:57,787
They also have a right to be here and a reason for being here.

