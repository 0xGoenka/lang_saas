1
00:00:00,000 --> 00:00:03,253
Dans les années 80, j'ai remarqué que

2
00:00:03,253 --> 00:00:05,505
parfois, lorsqu'un éléphant

3
00:00:05,505 --> 00:00:08,675
appelait un membre de sa famille,

4
00:00:08,675 --> 00:00:12,178
un individu répondait

5
00:00:12,178 --> 00:00:14,806
et que tout le monde ignorait l'animal qui l'appelait.

6
00:00:14,806 --> 00:00:17,934
Et puis elle appelait à nouveau,

7
00:00:17,934 --> 00:00:20,311
et un autre éléphant levait la

8
00:00:20,311 --> 00:00:24,399
tête et grondait très fort.

9
00:00:24,399 --> 00:00:25,692
C'est Joyce Poole.

10
00:00:25,692 --> 00:00:27,485
Elle étudie les éléphants d'Afrique

11
00:00:27,485 --> 00:00:30,030
et leur communication depuis 50 ans.

12
00:00:30,030 --> 00:00:31,990
Ensuite, j'ai commencé à penser, eh bien, d'accord,

13
00:00:31,990 --> 00:00:36,745
alors peut-être qu'ils ont un moyen de diriger un appel vers une personne en particulier.

14
00:00:36,745 --> 00:00:39,164
Mais nous n’avions aucun moyen de le détecter.

15
00:00:39,164 --> 00:00:41,791
Des décennies plus tard, elle s'est associée à Mickey Pardo,

16
00:00:41,791 --> 00:00:44,419
qui a conçu une étude autour de ses observations.

17
00:00:44,419 --> 00:00:45,420
Je suis allé sur le terrain.

18
00:00:45,420 --> 00:00:48,381
J'ai enregistré des appels avec des observations comportementales minutieuses.

19
00:00:48,381 --> 00:00:50,717
Nous savions donc qui passait chaque appel, nous savions à qui l'appel était

20
00:00:50,717 --> 00:00:53,136
adressé, nous connaissions le contexte de l'appel...

21
00:00:53,136 --> 00:00:56,014
Ils ont codé les informations acoustiques des enregistrements

22
00:00:56,014 --> 00:00:58,600
dans une longue chaîne de chiffres, ainsi que les données

23
00:00:58,600 --> 00:01:01,186
collectées par Mickey sur l'appel.  appels.

24
00:01:01,186 --> 00:01:03,104
Ils ont introduit près de 500

25
00:01:03,104 --> 00:01:05,690
appels différents comme celui-ci dans un modèle statistique.

26
00:01:05,690 --> 00:01:08,985
Et étant donné la structure acoustique d’un nouvel appel,

27
00:01:08,985 --> 00:01:11,738
le modèle pouvait prédire qui était le destinataire de l’appel,

28
00:01:11,738 --> 00:01:13,740
bien mieux que le hasard.

29
00:01:13,740 --> 00:01:15,909
En d’autres termes, des preuves suggèrent que les

30
00:01:15,909 --> 00:01:20,580
éléphants de savane africaine se donnent des noms.

31
00:01:20,580 --> 00:01:22,415
Lorsque nous l'avons publié sur Facebook,

32
00:01:22,415 --> 00:01:27,754
quelqu'un a répondu et a dit que la Terre avait juste bougé un petit peu.

33
00:01:27,754 --> 00:01:30,590
Et je pense que c'est vrai.

34
00:01:30,590 --> 00:01:32,801
Ce n’est là qu’un exemple de la façon dont l’apprentissage automatique

35
00:01:32,801 --> 00:01:37,138
décode les complexités de la communication animale
que les humains ne peuvent pas détecter.

36
00:01:37,138 --> 00:01:40,266
Et maintenant, certains chercheurs en IA souhaitent passer à l’étape suivante :

37
00:01:40,266 --> 00:01:43,520
de grands modèles de langage, comme ceux qui alimentent les chatbots,

38
00:01:43,520 --> 00:01:47,107
mais conçus pour la communication interspécifique.

39
00:01:47,107 --> 00:01:49,317
"Pouvons-nous parler un peu d'amour?"

40
00:01:49,317 --> 00:01:52,195
« Il reste encore beaucoup à apprendre sur les baleines. »

41
00:02:05,166 --> 00:02:07,127
Lorsque les chercheurs étudient la communication animale,

42
00:02:07,127 --> 00:02:09,211
ils emploient généralement quelques méthodes :

43
00:02:09,211 --> 00:02:11,548
enregistrer leurs vocalisations, observer

44
00:02:11,548 --> 00:02:14,676
et documenter le comportement et le contexte autour de ces sons,

45
00:02:14,676 --> 00:02:18,429
et parfois effectuer une lecture pour mesurer la réponse de l'animal.

46
00:02:18,429 --> 00:02:22,100
Tous ces domaines sont déjà touchés par l’IA.

47
00:02:22,100 --> 00:02:27,897
Les enregistrements sur le terrain ne ressemblent généralement pas à cela.

48
00:02:27,897 --> 00:02:32,443
Ils ressemblent souvent à ceci :

49
00:02:32,443 --> 00:02:33,570
plusieurs animaux

50
00:02:33,570 --> 00:02:36,948
vocalisant les uns sur les autres dans un environnement bruyant.

51
00:02:36,948 --> 00:02:39,075
C'est ce qu'on appelle le problème des cocktails,

52
00:02:39,075 --> 00:02:42,370
et c'est un problème courant dans le domaine de la recherche animale.

53
00:02:42,370 --> 00:02:46,583
Mais l’apprentissage automatique a résolu un problème similaire
en matière de reconnaissance vocale humaine.

54
00:02:46,583 --> 00:02:49,586
Les chercheurs en IA ont formé un modèle appelé Deep Karaoke sur

55
00:02:49,586 --> 00:02:52,088
de nombreuses pistes musicales où les instruments et les voix

56
00:02:52,088 --> 00:02:56,050
étaient enregistrés séparément, puis également sur les pistes entièrement mixées,

57
00:02:56,050 --> 00:02:59,387
jusqu'à ce qu'il soit capable de séparer les instruments

58
00:02:59,387 --> 00:03:02,348
et les voix dans de nouveaux clips musicaux.

59
00:03:02,348 --> 00:03:05,018
Récemment, des chercheurs en IA ont réussi à

60
00:03:05,018 --> 00:03:08,438
appliquer des algorithmes similaires aux enregistrements sonores d’animaux.

61
00:03:08,438 --> 00:03:11,941
Ce qui signifie que vous pouvez prendre un clip d’un groupe de singes macaques

62
00:03:11,941 --> 00:03:18,406
et distinguer un cri perceptible.

63
00:03:18,406 --> 00:03:20,325
Les chercheurs pourraient également commencer à utiliser

64
00:03:20,325 --> 00:03:22,869
l’IA dans la manière dont ils utilisent les lectures sur le terrain.

65
00:03:22,869 --> 00:03:25,205
Vous avez peut-être vu des modèles d'IA qui peuvent être entraînés

66
00:03:25,205 --> 00:03:28,875
sur de nombreux exemples d'enregistrement sonore,

67
00:03:31,628 --> 00:03:35,256
puis en générer une autre version unique.  Les

68
00:03:38,509 --> 00:03:39,969
chercheurs en IA commencent

69
00:03:39,969 --> 00:03:45,558
à développer des modèles similaires pour les enregistrements d’animaux.

70
00:03:45,558 --> 00:03:48,436
Ce sont tous des types d’« apprentissage supervisé ».

71
00:03:48,436 --> 00:03:49,979
Cela signifie que le modèle est formé

72
00:03:49,979 --> 00:03:52,482
sur de nombreux exemples étiquetés par des humains.

73
00:03:52,482 --> 00:03:54,692
Et dans l'étude sur les noms des éléphants, les chercheurs ont pu

74
00:03:54,692 --> 00:03:57,195
alimenter un modèle avec leurs observations, ce qui,

75
00:03:57,195 --> 00:03:59,322
avec les données sonores, les a aidés à détecter

76
00:03:59,322 --> 00:04:02,492
dans les cris des éléphants quelque chose qu'ils ne pouvaient pas détecter par la seule observation.

77
00:04:02,492 --> 00:04:04,285
Vous devez annoter beaucoup de données.

78
00:04:04,285 --> 00:04:06,746
Yossi Yovel a formé un modèle statistique sur

79
00:04:06,746 --> 00:04:09,874
15 000 vocalisations de chauves-souris frugivores égyptiennes,

80
00:04:09,874 --> 00:04:12,293
et il a pu identifier l'émetteur de l'appel,

81
00:04:12,293 --> 00:04:13,920
le contexte de l'appel,

82
00:04:13,920 --> 00:04:17,257
sa réponse comportementale et à qui l'appel était adressé.

83
00:04:17,257 --> 00:04:19,634
Et nous les avons annotés manuellement.

84
00:04:19,634 --> 00:04:22,720
Vous savez, je dis déjà que c’est une restriction de l’étude,

85
00:04:22,720 --> 00:04:25,807
parce que peut-être qu’il nous manque quelque chose, nous sommes des humains, nous ne sommes pas des chauves-souris.

86
00:04:25,807 --> 00:04:28,726
Et c’est le problème des modèles d’apprentissage supervisé.

87
00:04:28,726 --> 00:04:31,145
Ils sont limités par ce que nous, les humains, savons déjà

88
00:04:31,145 --> 00:04:34,732
sur la communication animale afin d'étiqueter les données d'entraînement.

89
00:04:34,732 --> 00:04:36,818
Et nous ne savons pas grand chose.

90
00:04:36,818 --> 00:04:40,154
C'est pourquoi certains chercheurs en IA affirment que les modèles auto-supervisés

91
00:04:40,154 --> 00:04:43,366
présentent le plus grand potentiel pour décoder la communication animale.

92
00:04:43,366 --> 00:04:47,287
C'est ainsi que sont formés les modèles de traitement du langage naturel comme ChatGPT.

93
00:04:47,287 --> 00:04:49,330
Au lieu d’exemples étiquetés par des humains,

94
00:04:49,330 --> 00:04:52,625
ils sont formés sur une grande quantité de données non étiquetées,

95
00:04:52,625 --> 00:04:55,545
puis peuvent les trier selon des modèles et des catégories qu’ils

96
00:04:55,545 --> 00:04:57,505
détectent eux-mêmes.

97
00:04:57,505 --> 00:05:00,633
Dans l’exemple de ChatGPT, il a appris de tous les livres,

98
00:05:00,633 --> 00:05:03,052
sites Web, flux de médias sociaux et de tout ce qu’il

99
00:05:03,052 --> 00:05:04,512
a pu extraire d’Internet,

100
00:05:04,512 --> 00:05:07,515
et est parvenu à ses propres conclusions sur le fonctionnement du langage.

101
00:05:07,515 --> 00:05:11,936
Chaque langage a une forme que l’IA découvre.

102
00:05:11,936 --> 00:05:13,187
C'est Aza Raskin.

103
00:05:13,187 --> 00:05:15,398
Il a cofondé le Earth Species Project,

104
00:05:15,398 --> 00:05:17,650
l'une des rares organisations qui souhaitent construire

105
00:05:17,650 --> 00:05:20,028
des modèles comme celui-ci pour la communication animale.

106
00:05:20,028 --> 00:05:22,655
Ce qu’il entend par langage ayant une forme,

107
00:05:22,655 --> 00:05:26,743
c’est que les modèles linguistiques sont construits à partir de relations entre les mots.

108
00:05:26,743 --> 00:05:29,245
Les mots qui signifient des choses similaires sont placés les uns à côté des autres, des mots

109
00:05:29,245 --> 00:05:33,624
qui partagent une relation, partagent une distance et une direction.

110
00:05:33,624 --> 00:05:36,127
Ainsi, l’homme est au roi ce que la femme est à la reine.

111
00:05:36,127 --> 00:05:38,796
Voilà donc la forme de toutes ces relations

112
00:05:38,796 --> 00:05:41,382
entre les 10 000 mots les plus courants de la langue anglaise

113
00:05:41,382 --> 00:05:44,385
, visualisées ici par le Earth Species Project.

114
00:05:44,385 --> 00:05:46,846
Aplati, cela ressemble à ceci.

115
00:05:46,846 --> 00:05:51,851
Quelque chose de vraiment miraculeux s'est produit en 2017 : les chercheurs

116
00:05:51,851 --> 00:05:54,979
ont découvert qu'on pouvait prendre la forme

117
00:05:54,979 --> 00:05:57,190
de n'importe quelle langue

118
00:05:57,190 --> 00:06:00,735
et la faire correspondre à la forme de n'importe quelle autre langue,

119
00:06:00,735 --> 00:06:03,780
et le point qui est « chien » se retrouve au même endroit.

120
00:06:03,780 --> 00:06:07,825
Cette idée, selon laquelle des mots similaires peuvent être localisés
dans d'autres langues à peu près au

121
00:06:07,825 --> 00:06:10,870
même endroit, est ce qui donne au Earth Species Project

122
00:06:10,870 --> 00:06:14,165
l'espoir que nous pourrions en faire une version pour la communication animale.

123
00:06:14,165 --> 00:06:17,168
Faire une traduction sans avoir besoin d’exemples,

124
00:06:17,168 --> 00:06:19,670
sans avoir besoin d’une pierre de Rosette.

125
00:06:19,670 --> 00:06:22,048
C’est cependant compliqué, car nous savons que les animaux

126
00:06:22,048 --> 00:06:25,259
ne communiquent pas seulement avec le son, mais aussi avec d’autres sens.

127
00:06:25,259 --> 00:06:27,512
Mais Aza souligne que nous pouvons apprendre du fait

128
00:06:27,512 --> 00:06:30,390
que les modèles de génération d’images comme DALL-E et Midjourney

129
00:06:30,390 --> 00:06:34,602
sont construits sur la même grande structure de modèle de langage que celle utilisée pour le texte.

130
00:06:34,602 --> 00:06:38,606
Il s’avère que, dans les coulisses, il s’agit encore de ce genre de formes.

131
00:06:38,606 --> 00:06:43,069
Il y a la forme qui représente
le son, la forme qui représente les images.

132
00:06:43,069 --> 00:06:46,364
Ces deux formes sont alignées et vous pouvez désormais traduire

133
00:06:46,364 --> 00:06:50,410
entre les images et le texte.

134
00:06:50,410 --> 00:06:52,620
Ils s’attendent à ce que l’

135
00:06:52,620 --> 00:06:55,164
alignement de la communication des animaux non humains sur la nôtre

136
00:06:55,164 --> 00:06:58,042
nous en dise encore plus sur ce que nous avons en commun.  Les

137
00:06:58,042 --> 00:07:01,421
dauphins se regardent dans les miroirs et se reconnaissent.

138
00:07:01,421 --> 00:07:02,255
Les éléphants aussi.

139
00:07:02,255 --> 00:07:04,298
C'est une sorte de conscience de soi.

140
00:07:04,298 --> 00:07:06,676
L’une des préoccupations de ce plan est liée à une étape

141
00:07:06,676 --> 00:07:09,554
de l’apprentissage auto-supervisé appelée validation, ce qui

142
00:07:09,554 --> 00:07:11,764
signifie que les humains doivent encore affiner ces modèles

143
00:07:11,764 --> 00:07:14,058
en les notant en fonction de leurs réponses.

144
00:07:14,058 --> 00:07:18,229
Comment ferions-nous cela dans une communication si étrangère à la nôtre ?

145
00:07:18,229 --> 00:07:21,732
Nous pourrions également avoir des attentes trop élevées à l’égard de ce chevauchement,

146
00:07:21,732 --> 00:07:23,943
ou de la capacité d’avoir une conversation

147
00:07:23,943 --> 00:07:28,114
avec un animal non humain dans un langage partagé
et sur des expériences partagées.

148
00:07:28,114 --> 00:07:30,074
"Hey Kurt, comment vas-tu mec?"

149
00:07:30,074 --> 00:07:32,618
"Alors je suis sur le point de traduire cela par un miaulement."

150
00:07:34,328 --> 00:07:35,830
«Nous nous sommes dit bonjour.  Salut.  Salut.  Salut.

151
00:07:35,830 --> 00:07:37,331


152
00:07:37,331 --> 00:07:40,501
Tu sais, la prochaine fois que tu voudras dire, comment vas-tu ?

153
00:07:40,501 --> 00:07:42,879
Je ne pense pas que les humains devraient être considérés comme

154
00:07:42,879 --> 00:07:46,215
plus importants que les autres espèces, mais cela ne veut pas dire

155
00:07:46,215 --> 00:07:50,052
qu'il n'est pas utile de faire la distinction entre le langage,

156
00:07:50,052 --> 00:07:53,097
qui est ce comportement très spécifique qui, du moins d'après ce que

157
00:07:53,097 --> 00:07:57,185
nous savons actuellement, semble être  propre aux humains

158
00:07:57,185 --> 00:07:59,353
et à d’autres formes de communication.

159
00:07:59,353 --> 00:08:02,064
Afin de construire ces modèles, la première étape consiste à collecter

160
00:08:02,064 --> 00:08:05,651
beaucoup plus de données sur les sons d’animaux qu’il n’en existe actuellement.

161
00:08:05,651 --> 00:08:09,697
Et donc je suis en train de constituer en ce moment une base de données

162
00:08:09,697 --> 00:08:12,158
avec tous les appels individuels.

163
00:08:12,158 --> 00:08:15,286
Donc, près de 10 000 enregistrements,

164
00:08:15,286 --> 00:08:18,372
ce qui est en fait très petit.

165
00:08:18,372 --> 00:08:19,957
Partout dans le monde, les chercheurs sur les animaux

166
00:08:19,957 --> 00:08:22,668
mènent un effort massif de collecte de données,

167
00:08:22,668 --> 00:08:25,630
marquant et enregistrant les animaux avec des données vidéo, sonores

168
00:08:25,630 --> 00:08:28,758
et spatiales pour alimenter ces modèles assoiffés de données.

169
00:08:28,758 --> 00:08:30,760
Le temps nous dira si la véritable

170
00:08:30,760 --> 00:08:33,554
communication interspécifique sera facilitée par l’IA.

171
00:08:33,554 --> 00:08:35,556
Mais les chercheurs espèrent que les découvertes en cours

172
00:08:35,556 --> 00:08:38,683
de route continueront d’avoir un impact sur notre appréciation

173
00:08:38,683 --> 00:08:42,354
et la protection des espèces avec lesquelles nous partageons la planète.

174
00:08:42,355 --> 00:08:46,692
Nous ne sommes pas les seuls sur la planète à pouvoir communiquer, à nous

175
00:08:46,692 --> 00:08:51,614
soucier les uns des autres, à avoir des pensées sur le passé

176
00:08:51,614 --> 00:08:52,949
et sur l'avenir.

177
00:08:52,949 --> 00:08:57,787
Ils ont également le droit d’être ici et une raison d’être ici.

